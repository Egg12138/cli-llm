#!/usr/bin/env python3
"""Simple CLI LLM client in a single file"""
import click
import openai
import time
import os
import re
import sys
import select
import signal
from pathlib import Path
from typing import Optional, Dict, Any
import logging
from logging.handlers import RotatingFileHandler
import os
from enum import Enum
from dataclasses import dataclass
import prompts
from prompts import SYS_ROLES
import tiktoken

# ============================================================================
# Module: Configuration
# ============================================================================

API_KEY = os.getenv("OPENAI_API_KEY")
API_ENDPOINT = os.getenv("OPENAI_BASE_URL")
DEFAULT_MODEL = os.getenv("OPENAI_MODEL")
print(API_ENDPOINT)

class ModelType(Enum):
    """Supported model types and their configurations."""
    CODER = "coder"
    CHAT = "chat"
    CREATIVE = "creative"
    CODER_REASONER = "coder-R"
    CHAT_REASONER = "chat-R"
    CREATIVE_REASONER = "creative-R"

    @property
    def config(self) -> Dict[str, Any]:
        return {
            ModelType.CODER: {'model_name': 'deepseek-coder', 'temp': 0.0},
            ModelType.CHAT: {'model_name': 'deepseek-chat', 'temp': 1.3},
            ModelType.CREATIVE: {'model_name': 'deepseek-chat', 'temp': 1.5},
            ModelType.CODER_REASONER: {'model_name': 'deepseek-reasoner', 'temp': 0.0},
            ModelType.CHAT_REASONER: {'model_name': 'deepseek-reasoner', 'temp': 1.3},
            ModelType.CREATIVE_REASONER: {'model_name': 'deepseek-reasoner', 'temp': 1.5},
        }[self]

@dataclass
class ColorCodes:
    """ANSI color codes for terminal output formatting."""
    RED: str = '\033[91m'
    BOLD: str = '\033[1m'
    BLUE: str = '\033[94m'
    MEGANT: str = '\033[35m'
    RESET: str = '\033[0m'
    BG_YELLOW: str = '\033[43m'

# Initialize color codes
COLORS = ColorCodes()
TIPF = f'{COLORS.BLUE}{COLORS.BOLD}'
CODEF = f'{COLORS.MEGANT}{COLORS.BOLD}'
ERRF = f'{COLORS.RED}{COLORS.BOLD}'
DESCF = f'{COLORS.BG_YELLOW}'
RSTF = f'{COLORS.RESET}'

HELP_TEXTS = {
    'prompt': f'{TIPF}Input prompt for deepseek.{RSTF}',
    'no_stream': f'{TIPF}Disable streaming of the response.(faster but need to wait until all results are generated){RSTF}, {ERRF}<role>-R does not support stream!{RSTF}',
    'model': f'{TIPF}Choose the model to use. Default is $OPENAI_MODEL.{RSTF}',
    'output_codes': f'{TIPF}Output the code to the target file. Only one code block is supported({DESCF}IN PROGRESS{RSTF})',
    'debug': f'{TIPF}Enable debug mode to show detailed logs.{RSTF}',
    'test': f'{TIPF}Do some local tests, will expand custom test functions future.{RSTF}',
    'role': f'{TIPF}Choose the role to use. Default is coder. Append an `-R` to use the reasoner of current mode, e.g. `coder-R`, `chat-R`, `creative-R`',
    'count_tokens': f'{TIPF}Enable token counting and show usage statistics.{RSTF}',
    'temp': f'{TIPF}Customize temperature value for the model. If not provided, uses the default for the selected role.{RSTF}',
}

# ============================================================================
# Module: Token Calculation
# ============================================================================

# Global token counters
input_tokens = 0
output_tokens = 0

def get_encoding_for_model(model: str) -> tiktoken.Encoding:
    """Get the appropriate encoding for a given model."""
    # Map models to their encoding types
    model_encoding_map = {
        'deepseek-coder': 'cl100k_base',
        'deepseek-chat': 'cl100k_base', 
        'deepseek-reasoner': 'cl100k_base',
        'gpt-4': 'cl100k_base',
        'gpt-3.5-turbo': 'cl100k_base',
        'gpt-4o': 'cl100k_base',
        'gpt-4o-mini': 'cl100k_base',
    }
    
    # Default to cl100k_base for most modern models
    encoding_name = model_encoding_map.get(model, 'cl100k_base')
    
    try:
        return tiktoken.get_encoding(encoding_name)
    except KeyError:
        # Fallback to cl100k_base if encoding not found
        return tiktoken.get_encoding('cl100k_base')

def count_tokens_in_messages(messages: list, model: str) -> int:
    """Count tokens in a list of messages."""
    encoding = get_encoding_for_model(model)
    total_tokens = 0
    
    for message in messages:
        # Count tokens in content
        if message.get('content'):
            total_tokens += len(encoding.encode(message['content']))
        
        # Add tokens for role formatting (typically 4 tokens per message)
        total_tokens += 4
        
        # Add tokens for name if present
        if message.get('name'):
            total_tokens += 1
    
    # Add tokens for assistant reply formatting
    total_tokens += 2
    
    return total_tokens

def count_tokens_in_text(text: str, model: str) -> int:
    """Count tokens in a single text string."""
    encoding = get_encoding_for_model(model)
    return len(encoding.encode(text))

def display_token_usage() -> None:
    """Display current token usage statistics."""
    total_tokens = input_tokens + output_tokens
    if total_tokens > 0:
        print(f"\n{TIPF}ðŸ“Š Token Usage:{RSTF}")
        print(f"  Input tokens: {input_tokens:,}")
        print(f"  Output tokens: {output_tokens:,}")
        print(f"  Total tokens: {total_tokens:,}")
        
        # Estimate cost (rough approximation - actual costs vary by model)
        estimated_cost = (input_tokens * 0.00001) + (output_tokens * 0.00003)
        print(f"  Estimated cost: ~${estimated_cost:.4f}")
    else:
        print(f"\n{TIPF}ðŸ“Š No tokens counted yet. Use --count-tokens to enable token counting.{RSTF}")

def resolve_key(key: Optional[str]) -> str:
    if key is None:
        logger.warning(f"used empty key")
        return "dummy-key"
    return key



# ============================================================================
# Module: Logging
# ============================================================================

def setup_logging(log_file: str = "/var/log/cli_llm.log", 
                  max_bytes: int = 1024*1024,
                  backup_count: int = 5) -> None:
    log_dir = os.path.dirname(log_file)
    path = Path(log_file)
    try:
        path.touch(exist_ok=True)
    except PermissionError:
        log_dir = os.path.expanduser('~/.cli_llm/logs')
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, 'cli_llm.log')

    file_handler = RotatingFileHandler(
        log_file,
        maxBytes=max_bytes,
        backupCount=backup_count
    )
    
    formatter = logging.Formatter('[%(levelname)s] - %(message)s')
    file_handler.setFormatter(formatter)
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    
    # Console handler for interactive sessions
    if sys.stdout.isatty():
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

setup_logging()
logger = logging.getLogger(__name__)

# ============================================================================
# Module: Text Processing
# ============================================================================

def highlight_code_blocks(content: str, session_type: str = 'Context') -> str:
    """Highlight code blocks and bold text with ANSI escape sequences."""
    # First handle bold text with **TEXT** format
    content = re.sub(
        r'\*\*(.*?)\*\*',
        lambda m: f'{COLORS.BOLD}{COLORS.BLUE}{m.group(1)}{COLORS.RESET}',
        content,
        flags=re.DOTALL
    )
    
    def replacer(matcher):
        return f'{CODEF}{matcher.group(0)}{RSTF}'
    
    if session_type in ('Context', 'Reasoning'):
        return re.sub(r'`(.*?)`', replacer, content, flags=re.DOTALL)
    return content

def sanitize_input(input_str: str) -> str:
    """Clean special characters from the input string."""
    sanitized_str = re.sub(r'[\x00-\x1F\x7F-\x9F\uD800-\uDFFF]', '', input_str)
    logger.debug(f"Input cleaned: {input_str[:50]}...")
    return sanitized_str

# ============================================================================
# Module: Response Processing
# ============================================================================

def process_streamed_chunk(response, count_tokens: bool = False, extra_session_type: Optional[str] = None) -> None:
    """Process and display streamed response chunks."""
    global output_tokens
    
    in_code_block = False
    full_response_content = ""
    
    for chunk in response:
        content = chunk.choices[0].delta.content if chunk.choices else None
        if content:
            if count_tokens:
                full_response_content += content
            content = re.sub(
                r'\*\*(.*?)\*\*',
                lambda m: f'{COLORS.BOLD}{COLORS.BLUE}{m.group(1)}{COLORS.RESET}',
                content
            )

            is_code = '`' in content
            if is_code and not in_code_block:
                in_code_block = True
                print(f'{CODEF}{content}', end='')
            elif is_code and in_code_block:
                in_code_block = False
                print(f'{CODEF}{content}{RSTF}', end='')
            elif not is_code and in_code_block:
                print(f'{CODEF}{content}', end='')
            else:
                print(f'{RSTF}{content}', end='')
    
    # Calculate output tokens from the full response content only if token counting is enabled
    if count_tokens and full_response_content:
        # Get model name from the first chunk
        model_name = response.model if hasattr(response, 'model') else 'deepseek-chat'
        output_token_count = count_tokens_in_text(full_response_content, model_name)
        output_tokens += output_token_count
        logger.info(f"ðŸ“Š Streamed output tokens: {output_token_count}")

def process_unstreamed_chunk(response, response_time: float, count_tokens: bool = False, extra_session_type: Optional[str] = None) -> None:
    """Process and display non-streamed response."""
    global output_tokens
    
    choice = response.choices[0]
    modelname = response.model
    finish_reason = choice.finish_reason
    finish_reason_map = {
        'stop': 'Normal',
        'length': 'Length exceeded max_tokens limit',
        'content_filter': 'Content filter triggered',
        'insufficient_system_resource': 'Insufficient system resources',
    }
    status = finish_reason_map.get(finish_reason, 'Unknown')

    if extra_session_type == 'Reasoning':
        print(f"{TIPF}@ {modelname} reasoning ========================================={RSTF}")
        
    answer = highlight_code_blocks(choice.message.content)
    print(answer)
    print(f"{TIPF}@ {modelname} [{status}] Response time: {response_time:.2f}s:{RSTF}")
    
    # Calculate output tokens from response only if token counting is enabled
    if count_tokens:
        if hasattr(response, 'usage') and response.usage:
            output_token_count = response.usage.completion_tokens
            output_tokens += output_token_count
            logger.info(f"ðŸ“Š Output tokens: {output_token_count}")
        else:
            # Fallback: estimate output tokens from response content
            response_content = choice.message.content
            output_token_count = count_tokens_in_text(response_content, modelname)
            output_tokens += output_token_count
            logger.info(f"ðŸ“Š Estimated output tokens: {output_token_count}")

# ============================================================================
# Module: Signal Handling
# ============================================================================

def sigint_handler(sig: int, frame: Any) -> None:
    """Handle SIGINT signal gracefully."""
    print(f'\n{TIPF}SIGINT received. Exiting...{RSTF}')
    # Only show token usage if we have any tokens counted
    if input_tokens > 0 or output_tokens > 0:
        display_token_usage()
    sys.exit(0)

def ensure_url_parser_ok() -> None:
    """Set up URL parsing related environment variables and signal handling."""
    signal.signal(signal.SIGINT, sigint_handler)
    os.environ['NO_PROXY'] = 'localhost'
    logger.info("URL parser configured successfully")

# ============================================================================
# Module: Core Chat Functionality
# ============================================================================

def chat(prompt: str, no_stream: bool, model: str, sys_role: prompts.SystemPrompt, client: Any, count_tokens: bool = False, custom_temp: Optional[float] = None) -> None:
    global input_tokens, output_tokens
    
    role = sys_role.content
    # Use custom temperature if provided, otherwise use the default for the role
    temp = custom_temp if custom_temp is not None else sys_role.temperature
    msg = [
        {"role": "system", "content": role},
        {"role": "user", "content": prompt},
    ]
    
    # Calculate input tokens only if token counting is enabled
    input_token_count = 0
    if count_tokens:
        input_token_count = count_tokens_in_messages(msg, model)
        input_tokens += input_token_count
    
    start_time = time.time()
    try:
        logger.info(f"ðŸš€ Request to {model} ({'streaming output' if not no_stream else 'âºï¸ non-streaming'})")
        if count_tokens:
            logger.info(f"ðŸ“Š Input tokens: {input_token_count}")
        
        if not no_stream:
            response = client.chat.completions.create(
                model=model,
                messages=msg,
                temperature=temp,
                stream=True,
            )
            print(f'{TIPF} ðŸ’­Generating...{RSTF}')
            process_streamed_chunk(response, count_tokens)
        else:
            response = client.chat.completions.create(
                model=model,
                messages=msg,
                temperature=temp,
            )
            process_unstreamed_chunk(response, time.time() - start_time, 'Reasoning', count_tokens)
        
        response_time = time.time() - start_time
        
        logger.info(f"âœ… Response completed in {response_time:.2f}s")
        print(f"\n{TIPF}â±ï¸ Response time: {response_time:.2f}s{RSTF}")

    except openai.APIError as e:
        logger.error(f"âš ï¸ OpenAI API Error: {e}", exc_info=True)
        try:
            error_status_code = int(e.message.split('-')[0].strip()[-3:])
        except ValueError as ve:
            error_status_code = e.message  

        error_type = e.type
        print(f"{ERRF}âŒ Error type: {error_type} (Code: {error_status_code}){RSTF}")

# ============================================================================
# Module: CLI Interface
# ============================================================================

def test_result():
    """Run test functions."""
    print(f"{prompts.SystemPrompt=}")
    print(f"{prompts.CODER_PROMPT=}")
    print("test something")

def get_sys_role(role: str) -> prompts.SystemPrompt:

    sys_role_marker = role.split('-')[0] if role in SYS_ROLES.keys() else 'coder'
    
    return SYS_ROLES[sys_role_marker]

@click.command()
@click.argument('prompt', required=False, default=None)
@click.option('-n', '--no-stream', is_flag=True, help=HELP_TEXTS['no_stream'])
@click.option('-r', '--role', default='coder', help=HELP_TEXTS['role'])
@click.option('-m', '--model', help=HELP_TEXTS['model'])
@click.option('-t', '--temp', type=float, help=HELP_TEXTS['temp'])
@click.option('-o', '--output-codes', nargs=1, default=None, help=HELP_TEXTS['output_codes'])
@click.option('-d', '--debug', is_flag=True, 
              help=HELP_TEXTS['debug'], default=False)
@click.option('--localtest', is_flag=True, 
              help=HELP_TEXTS['test'], default=False)
@click.option('--count-tokens', is_flag=True, help=HELP_TEXTS['count_tokens'])
def chat_cli(prompt: Optional[str], no_stream: bool, model: Optional[str], role: Optional[str], temp: Optional[float], output_codes: Optional[str], debug: bool, localtest: bool, count_tokens: bool) -> None:

    
    # handle missing options
    if prompt is None:
        prompt = input(f'{TIPF}[Ask]:{RSTF}')
    prompt = sanitize_input(prompt)

    if model is None:
        model = DEFAULT_MODEL
    
    if select.select([sys.stdin], [], [], 0.0)[0]:
        stdin_input = sys.stdin.read().strip()
    else:
        stdin_input = ""
    
    full_prompt = '\n'.join([prompt, stdin_input])
    ensure_url_parser_ok()
    
    # Handle empty API key - some local LLM servers don't require it
    api_key = resolve_key(API_KEY)
    client = openai.OpenAI(api_key=api_key, base_url=API_ENDPOINT)
    
    sys_role = get_sys_role(role)
    
    if debug:
        logger.setLevel(logging.DEBUG)
        for handler in logger.handlers:
            handler.setLevel(logging.DEBUG)
    
    if localtest:
        return test_result()

    chat(full_prompt, no_stream, model, sys_role, client, count_tokens, temp)

def main() -> None:
    try:
        chat_cli()
        # Show token usage if we have any tokens counted
        if input_tokens > 0 or output_tokens > 0:
            display_token_usage()
    except Exception as e:
        logger.error(f"Application error: {e}", exc_info=True)
        print(f"{ERRF}fatal: {str(e)}{RSTF}")
        # Show token usage if we have any tokens counted
        if input_tokens > 0 or output_tokens > 0:
            display_token_usage()
        sys.exit(1)

if __name__ == '__main__':
    main()
